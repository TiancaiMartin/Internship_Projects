{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# 读取数据\n",
    "sec_post_1d_df = pd.read_pickle(r'/Users/tiancaixiaohuoban/Desktop/实习/买方实习/中信建投期货（金融工程）/策略设计/dom_cont_post_1d')\n",
    "# sec_post_1d_df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e78f522af972c6ed",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "univ01 = ['AG', 'AL', 'NI', 'ZN',\n",
    "          'RB', 'HC', 'I', 'J', 'JM', 'FG',\n",
    "          'BU', 'L', 'MA', 'PP', 'RU', 'V', 'TA', 'FU', 'EG', 'SP',\n",
    "          'A', 'M', 'P', 'OI', 'Y', 'SR', 'CF', 'C', 'RM']\n",
    "\n",
    "univ02 = ['AG', 'AL', 'NI', 'ZN',\n",
    "          'RB', 'HC', 'I', 'J', 'JM', 'FG',\n",
    "          'BU', 'L', 'MA', 'PP', 'RU', 'V', 'TA', 'FU', 'EG', 'SP',\n",
    "          'A', 'M', 'P', 'OI', 'Y', 'SR', 'CF', 'C', 'RM', 'CU', 'PB', 'SN', 'SF', 'SM', 'AP', 'SA']\n",
    "\n",
    "univ_nonferrous = ['CU', 'AL', 'NI', 'ZN', 'PB', 'SN']\n",
    "univ_black = ['RB', 'I', 'HC', 'J', 'JM', 'FG', 'SF', 'SM', 'SA']\n",
    "univ_chemic = ['FU', 'BU', 'RU', 'L', 'TA', 'V', 'EG', 'MA', 'PP']\n",
    "univ_agri = ['A', 'AP', 'C', 'CF', 'CS', 'JD', 'M', 'OI', 'P', 'RM', 'SR', 'Y']\n",
    "univ_precious = ['AU', 'AG']"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5b0b48df16d2609e",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def kagi(P, H):\n",
    "    '''\n",
    "    create kagi contrusction for process P with parameter H\n",
    "    \n",
    "    Args:\n",
    "        P (numpy.ndarray): input time series \n",
    "        H (float): parameter of kagi construction\n",
    "            \n",
    "    Returns:\n",
    "        tau_a (list): time moments when process P has local maximum or minimum\n",
    "        tau_b (list): time moments when local maximum or minimum is recognized\n",
    "    '''\n",
    "    \n",
    "    tau_a = []\n",
    "    tau_b = []\n",
    "    \n",
    "    # find tau_b[0]\n",
    "    for u in range(0,len(P)-1):\n",
    "        if (np.max(P[0:u+1]) - np.min(P[0:u+1])) > H:\n",
    "            tau_b.append(u)\n",
    "            break\n",
    "    \n",
    "    # if not found, return empty lists\n",
    "    if len(tau_b)<1:\n",
    "        return tau_a, tau_b\n",
    "\n",
    "    # find tau_a[0]\n",
    "    for u in range(0,tau_b[0]):\n",
    "        if abs(P[u] - P[tau_b[0]]) > H:\n",
    "            tau_a.append(u)\n",
    "            break\n",
    "    # if not found, return empty lists\n",
    "    if len(tau_a)<1:\n",
    "        return tau_a, tau_b\n",
    "\n",
    "    # determine if tau_a[0] is min or max\n",
    "    S_0 = np.sign(P[tau_a[0]] - P[tau_b[0]])\n",
    "\n",
    "    if S_0==1: # if min\n",
    "        n=1\n",
    "    else: # if max\n",
    "        n=0\n",
    "    \n",
    "    # process time series P\n",
    "    finished = False\n",
    "    while not finished:\n",
    "        finished = True\n",
    "        \n",
    "        # if local minimum\n",
    "        if n%2==1:\n",
    "            # find tau_b\n",
    "            for u in range(tau_a[-1]+1,len(P)):\n",
    "                if (P[u] - np.min(P[tau_a[-1]:u+1])) > H:\n",
    "                    if u > tau_b[-1]:\n",
    "                        tau_b.append(u)\n",
    "                        finished = False\n",
    "                        break\n",
    "                    else:\n",
    "                        finished = True\n",
    "            # find tau_a\n",
    "            if not finished:\n",
    "                argmin = np.argmin(P[tau_a[-1]:tau_b[-1]+1])\n",
    "                tau_a.append(tau_a[-1]+argmin)\n",
    "                n = n+1\n",
    "        \n",
    "        # if local maximum\n",
    "        elif n%2==0:\n",
    "            # find tau_b\n",
    "            for u in range(tau_a[-1]+1,len(P)):\n",
    "                if (np.max(P[tau_a[-1]:u+1]) - P[u]) > H:\n",
    "                    if u > tau_b[-1]:\n",
    "                        tau_b.append(u)\n",
    "                        finished=False\n",
    "                        break\n",
    "                    else:\n",
    "                        finished = True\n",
    "            # find tau_a\n",
    "            if not finished:\n",
    "                argmax = np.argmax(P[tau_a[-1]:tau_b[-1]+1])\n",
    "                tau_a.append(tau_a[-1]+argmax)\n",
    "                n = n+1\n",
    "                \n",
    "    return tau_a, tau_b"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "16e668fb963904d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def load_data(univ, sec_post_1d_df):\n",
    "    # 从原始数据中提取出univ中的合约的收盘价数据\n",
    "    data = pd.DataFrame()\n",
    "    for contract in univ:\n",
    "        df = sec_post_1d_df.loc[contract].T\n",
    "        data[contract] = df['close']\n",
    "    \n",
    "    return data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "194953802a7e20d1",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def parse_pair(pair):\n",
    "    s1 = pair[:pair.find('-')]\n",
    "    s2 = pair[pair.find('-')+1:]\n",
    "    return s1,s2"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a17de6b140ff1766",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def h_volatility(P, tau_a, tau_b):\n",
    "    '''\n",
    "    Calcuate H-volatility and H-inversion for process P with given times tau_a and tau_b\n",
    "    \n",
    "    Args:\n",
    "        P (numpy.ndarray): input time series \n",
    "        tau_a (list): time moments when process P has local maximum or minimum\n",
    "        tau_b (list): time moments when local maximum or minimum is recognized\n",
    "            \n",
    "    Returns:\n",
    "        h_vol (float): H-volatility of order 1\n",
    "        h_inv (int): H-inversion\n",
    "    '''\n",
    "\n",
    "    h_inv = len(tau_b)\n",
    "    V = abs(np.diff(P[tau_a])).sum()\n",
    "    h_vol = V / h_inv\n",
    "\n",
    "    return h_vol, h_inv"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ac3e7d434efc327f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def rolling_generate_signals1(prices, drift, rolling_window, pairs_num, flag):\n",
    "    # generate signals for rolling window\n",
    "    train_begin = datetime.strptime('2010-01-01','%Y-%m-%d') + timedelta(days=drift)\n",
    "    train_begin = train_begin.strftime('%Y-%m-%d')\n",
    "    train_end = datetime.strptime(train_begin,'%Y-%m-%d') + timedelta(days=rolling_window-1)\n",
    "    train_end = train_end.strftime('%Y-%m-%d')\n",
    "    test_begin = datetime.strptime(train_end,'%Y-%m-%d') + timedelta(days=1)\n",
    "    test_begin = test_begin.strftime('%Y-%m-%d')\n",
    "    test_end = datetime.strptime(test_begin,'%Y-%m-%d') + timedelta(days=21)\n",
    "    test_end = test_end.strftime('%Y-%m-%d')\n",
    "    prices_train = prices.loc[train_begin:train_end]\n",
    "    prices_train = prices_train.dropna(axis=1,how='any')\n",
    "    log_prices_train = np.log(prices_train)\n",
    "    pairs_df = pd.DataFrame(columns=['H', 'H-volatility','H-inversion', 'Last extremum'])\n",
    "    selected_pairs = []\n",
    "    selected_stocks = []\n",
    "    \n",
    "    # generate pairs_df\n",
    "    for s1 in log_prices_train.columns:\n",
    "        for s2 in log_prices_train.columns:\n",
    "            if (s1 != s2) and (f'{s2}-{s1}' not in pairs_df.index):\n",
    "                spread = (log_prices_train[s1] - log_prices_train[s2]).values\n",
    "                H = 0.9*spread.std()\n",
    "                tau_a, tau_b = kagi(spread, H)\n",
    "    \n",
    "                if len(tau_a) < 1:\n",
    "                    pairs_df.loc[f'{s1}-{s2}'] = -1, -1, -1, -1  # if no local min\\max found\n",
    "                else:\n",
    "                    h_vol, h_inv = h_volatility(spread, tau_a, tau_b)\n",
    "    \n",
    "                    if spread[tau_b[-1]] > spread[tau_a[-1]]:\n",
    "                        last_extr = 'min'\n",
    "                    else:\n",
    "                        last_extr = 'max'\n",
    "    \n",
    "                    pairs_df.loc[f'{s1}-{s2}'] = H, h_vol, h_inv, last_extr\n",
    "    for pair in pairs_df.sort_values(by='H-inversion', ascending=False).index:\n",
    "        s1,s2 = parse_pair(pair)\n",
    "        if (s1 not in selected_stocks) and (s2 not in selected_stocks):\n",
    "            selected_pairs.append(pair)\n",
    "            selected_stocks.append(s1)\n",
    "            selected_stocks.append(s2)\n",
    "        if len(selected_pairs)==pairs_num:\n",
    "            break\n",
    "    # print(selected_pairs)\n",
    "    # generate returns\n",
    "    prices_test = prices.loc[test_begin:test_end]\n",
    "    prices_test = prices_test.dropna(axis=1,how='any')\n",
    "    returns_test = prices_test.pct_change()\n",
    "    log_prices_test = np.log(prices_test)\n",
    "    \n",
    "    # generate signals\n",
    "    positions = pd.DataFrame(index=returns_test.index, columns=selected_stocks)\n",
    "    for t in log_prices_test.index:\n",
    "        log_prices_tmp = log_prices_test.loc[:t]\n",
    "        log_prices_tmp = log_prices_tmp.iloc[-21:]\n",
    "    \n",
    "        for pair in selected_pairs:\n",
    "            s1, s2 = parse_pair(pair)\n",
    "            spread = (log_prices_tmp[s1] - log_prices_tmp[s2]).values\n",
    "            H = pairs_df.loc[pair, 'H']\n",
    "            h_vol = pairs_df.loc[pair, 'H-volatility']\n",
    "            tau_a, tau_b = kagi(spread, H)\n",
    "            if not tau_a:  # no local min\\max found\n",
    "                tau_a = [0]\n",
    "            if not tau_b:\n",
    "                tau_b = [0]\n",
    "    \n",
    "            if tau_b[-1] == (len(spread) - 1):  # new local extremum detected\n",
    "                if spread[tau_a[-1]] > spread[tau_b[-1]]:  # spread moves down\n",
    "                    if h_vol > 1.4 * H:  # trend following\n",
    "                        positions.loc[t, [s1, s2]] = [-1, 1]\n",
    "                    else:  # contrarian\n",
    "                        positions.loc[t, [s1, s2]] = [1, -1]\n",
    "                else:  # spread moves up\n",
    "                    if h_vol > 1.4 * H:  # trend following\n",
    "                        positions.loc[t, [s1, s2]] = [1, -1]\n",
    "                    else:  # contrarian\n",
    "                        positions.loc[t, [s1, s2]] = [-1, 1]\n",
    "    positions.fillna(method='ffill', inplace=True)\n",
    "    # add positions at the beginning of the trading period\n",
    "    for pair in selected_pairs:\n",
    "        s1,s2 = parse_pair(pair)\n",
    "        H = pairs_df.loc[pair, 'H']\n",
    "        h_vol = pairs_df.loc[pair, 'H-volatility']\n",
    "        last_extr = pairs_df.loc[pair, 'Last extremum']\n",
    "        if h_vol > 1.4*H: # trend following\n",
    "            if last_extr=='min':\n",
    "                # short position\n",
    "                positions[s1].fillna(-1, inplace=True) \n",
    "                positions[s2].fillna(1, inplace=True)\n",
    "            else:\n",
    "                # long position\n",
    "                positions[s1].fillna(1, inplace=True) \n",
    "                positions[s2].fillna(-1, inplace=True)\n",
    "        else: # contrarian\n",
    "            if last_extr=='min':\n",
    "                # long position\n",
    "                positions[s1].fillna(1, inplace=True) \n",
    "                positions[s2].fillna(-1, inplace=True)\n",
    "            else:\n",
    "                # short position\n",
    "                positions[s1].fillna(-1, inplace=True) \n",
    "                positions[s2].fillna(1, inplace=True)\n",
    "    positions.fillna(method='ffill', inplace=True)\n",
    "    positions.fillna(0, inplace=True)\n",
    "    if(flag):\n",
    "        return positions\n",
    "    return positions.tail(1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7909855b9ef11d1a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def rolling_generate_signals2(prices, drift, rolling_window, pairs_num,flag):\n",
    "    # generate signals for rolling window\n",
    "    train_begin = datetime.strptime('2010-01-01','%Y-%m-%d') + timedelta(days=drift)\n",
    "    train_begin = train_begin.strftime('%Y-%m-%d')\n",
    "    train_end = datetime.strptime(train_begin,'%Y-%m-%d') + timedelta(days=rolling_window-1)\n",
    "    train_end = train_end.strftime('%Y-%m-%d')\n",
    "    test_begin = datetime.strptime(train_end,'%Y-%m-%d') + timedelta(days=1)\n",
    "    test_begin = test_begin.strftime('%Y-%m-%d')\n",
    "    test_end = datetime.strptime(test_begin,'%Y-%m-%d') + timedelta(days=21)\n",
    "    test_end = test_end.strftime('%Y-%m-%d')\n",
    "    prices_train = prices.loc[train_begin:train_end]\n",
    "    prices_train = prices_train.dropna(axis=1,how='any')\n",
    "    log_prices_train = np.log(prices_train)\n",
    "    pairs_df = pd.DataFrame(columns=['H', 'H-volatility','H-inversion', 'Last extremum'])\n",
    "    selected_pairs = []\n",
    "    selected_stocks = []\n",
    "    \n",
    "    # generate pairs_df\n",
    "    for s1 in log_prices_train.columns:\n",
    "        for s2 in log_prices_train.columns:\n",
    "            if (s1 != s2) and (f'{s2}-{s1}' not in pairs_df.index):\n",
    "                spread = (log_prices_train[s1] - log_prices_train[s2]).values\n",
    "                H = 0.9*spread.std()\n",
    "                tau_a, tau_b = kagi(spread, H)\n",
    "    \n",
    "                if len(tau_a) < 1:\n",
    "                    pairs_df.loc[f'{s1}-{s2}'] = -1, -1, -1, -1  # if no local min\\max found\n",
    "                else:\n",
    "                    h_vol, h_inv = h_volatility(spread, tau_a, tau_b)\n",
    "    \n",
    "                    if spread[tau_b[-1]] > spread[tau_a[-1]]:\n",
    "                        last_extr = 'min'\n",
    "                    else:\n",
    "                        last_extr = 'max'\n",
    "    \n",
    "                    pairs_df.loc[f'{s1}-{s2}'] = H, h_vol, h_inv, last_extr\n",
    "    for pair in pairs_df.sort_values(by='H-inversion', ascending=False).index:\n",
    "        s1,s2 = parse_pair(pair)\n",
    "        if (s1 not in selected_stocks) and (s2 not in selected_stocks):\n",
    "            selected_pairs.append(pair)\n",
    "            selected_stocks.append(s1)\n",
    "            selected_stocks.append(s2)\n",
    "        if len(selected_pairs)==pairs_num:\n",
    "            break\n",
    "    # print(selected_pairs)\n",
    "    # generate returns\n",
    "    prices_test = prices.loc[test_begin:test_end]\n",
    "    prices_test = prices_test.dropna(axis=1,how='any')\n",
    "    returns_test = prices_test.pct_change()\n",
    "    log_prices_test = np.log(prices_test)\n",
    "    \n",
    "    # generate signals\n",
    "    positions = pd.DataFrame(index=returns_test.index, columns=selected_stocks)\n",
    "    for t in log_prices_test.index:\n",
    "        log_prices_tmp = log_prices_test.loc[:t]\n",
    "        log_prices_tmp = log_prices_tmp.iloc[-21:]\n",
    "    \n",
    "        for pair in selected_pairs:\n",
    "            # 提取pair中的两个合约\n",
    "            s1,s2 = parse_pair(pair)\n",
    "            # 计算spread\n",
    "            spread = (log_prices_tmp[s1] - log_prices_tmp[s2]).values\n",
    "            #H = pairs_df.loc[pair, 'H']\n",
    "            #h_vol = pairs_df.loc[pair, 'H-volatility']\n",
    "            H = 0.9*spread.std()\n",
    "            tau_a,tau_b = kagi(spread, H)\n",
    "            if not tau_a:  # no local min\\max found\n",
    "                tau_a = [0]\n",
    "            if not tau_b:\n",
    "                tau_b = [0]\n",
    "            h_vol, h_inv = h_volatility(spread, tau_a, tau_b)\n",
    "            \n",
    "            if tau_b[-1] == (len(spread)-1):# 若发现新的极值点\n",
    "                if spread[tau_a[-1]] > spread[tau_b[-1]]: # 价差增大\n",
    "                    if h_vol > 1.4*H: # 趋势跟踪\n",
    "                        positions.loc[t, [s1,s2]] = [-1,1]\n",
    "                    else: # contrarian\n",
    "                        positions.loc[t, [s1,s2]] = [1,-1]\n",
    "                else: # 价差减少\n",
    "                    if h_vol > 1.4*H: # 趋势跟踪\n",
    "                        positions.loc[t, [s1,s2]] = [1,-1]\n",
    "                    else: # contrarian\n",
    "                        positions.loc[t, [s1,s2]] = [-1,1]\n",
    "    positions.fillna(method='ffill', inplace=True)\n",
    "    positions.fillna(0, inplace=True)\n",
    "    if(flag):\n",
    "        return positions\n",
    "    # 返回positions的最后一行\n",
    "    return positions.tail(1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fbe924a1ef3187df",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def calculate_metrics(cumret):\n",
    "    '''\n",
    "    calculate performance metrics from cumulative returns\n",
    "    '''\n",
    "    total_return = (cumret[-1] - cumret[0])/cumret[0]\n",
    "    apr = (1+total_return)**(252/len(cumret)) - 1\n",
    "    rets = pd.DataFrame(cumret).pct_change()\n",
    "    sharpe = np.sqrt(21) * np.nanmean(rets) / np.nanstd(rets)\n",
    "    \n",
    "    # maxdd and maxddd\n",
    "    highwatermark=np.zeros(cumret.shape)\n",
    "    drawdown=np.zeros(cumret.shape)\n",
    "    drawdownduration=np.zeros(cumret.shape)\n",
    "    for t in np.arange(1, cumret.shape[0]):\n",
    "        highwatermark[t]=np.maximum(highwatermark[t-1], cumret[t])\n",
    "        drawdown[t]=cumret[t]/highwatermark[t]-1\n",
    "        if drawdown[t]==0:\n",
    "            drawdownduration[t]=0\n",
    "        else:\n",
    "            drawdownduration[t]=drawdownduration[t-1]+1\n",
    "    maxDD=np.min(drawdown)\n",
    "    maxDDD=np.max(drawdownduration)\n",
    "    \n",
    "    return total_return, apr, sharpe, maxDD, maxDDD"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "97e0d91b5ceee5a8",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# # 定义两个日期\n",
    "# date1 = datetime(2010, 1, 1)\n",
    "# date2 = datetime(2024, 1, 25)\n",
    "# # 计算间隔天数\n",
    "# interval = (date2 - date1).days\n",
    "# interval"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c72561d8c9412d7",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## strategy 1.3 pairs"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "533d493d1ab35fff"
  },
  {
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "prices = load_data(univ02, sec_post_1d_df)\n",
    "# prices = prices.dropna()\n",
    "for drift in tqdm(range(0, 5011)):\n",
    "    # merge positions\n",
    "    if drift == 0:\n",
    "        positions = rolling_generate_signals1(prices, drift, 126, 3, 1)\n",
    "        positions_all = positions\n",
    "        continue\n",
    "    else:\n",
    "        positions = rolling_generate_signals1(prices, drift, 126, 3, 0)\n",
    "    positions_all = pd.concat([positions_all, positions], axis=0)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a78ce18c8643709",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "positions_all"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e3c437b66fa64a7a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "positions_all.fillna(0)\n",
    "# 找出重复的索引\n",
    "duplicated_index = positions_all.index.duplicated(keep='first')\n",
    "# 删除具有重复索引的列\n",
    "positions_all = positions_all[~duplicated_index]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e6256705425791ed",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# 将position_all中的int类型转换为float类型\n",
    "for col in positions_all.columns:\n",
    "    positions_all[col] = positions_all[col].astype(float)\n",
    "test_signal_df = positions_all.T\n",
    "test_signal_df = test_signal_df.fillna(0)\n",
    "test_signal_df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "97b488c04428fae8",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def backtest(positions, fee_rate, univ, start_date, end_date, pair_num, strategy_name):\n",
    "    prices = load_data(univ, sec_post_1d_df)\n",
    "    # calculate returns\n",
    "    prices_test = prices.loc[start_date:end_date]\n",
    "    returns_test = prices_test.pct_change()\n",
    "    positions = positions.loc[start_date:end_date]\n",
    "    # calculate fee\n",
    "    fee = positions.diff().abs() * fee_rate\n",
    "    ret = ((positions.shift() * returns_test[univ])-fee).sum(axis=1) / pair_num * 2\n",
    "    cumret = np.nancumprod(ret+1)\n",
    "    cumret = pd.Series(cumret, index=ret.index)\n",
    "    columns = ['Total return', 'APR', 'Sharpe ratio', 'Max Drawdown', 'Max Drawdown Duration']\n",
    "    performance_df = pd.DataFrame(columns=columns)\n",
    "    performance_df.loc[strategy_name] = calculate_metrics(cumret)\n",
    "    print(performance_df)\n",
    "    plt.figure(figsize=(18,4))\n",
    "    plt.plot(cumret, label='strategy cumulative returns')\n",
    "    plt.legend()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e9a7bc4cfd435313",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "backtest(positions_all,0.0001,univ02,'2010-05-07','2024-01-25',3,'strategy 1.3 pairs')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fee2d02d1dacd297",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## strategy 2.3 pairs"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b154afbfd66fcb3"
  },
  {
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "prices = load_data(univ02, sec_post_1d_df)\n",
    "# prices = prices.dropna()\n",
    "for drift in tqdm(range(0, 5011)):\n",
    "    # merge positions\n",
    "    if drift == 0:\n",
    "        positions = rolling_generate_signals2(prices, drift, 126, 3, 1)\n",
    "        positions_all = positions\n",
    "        continue\n",
    "    else:\n",
    "        positions = rolling_generate_signals2(prices, drift, 126, 3, 0)\n",
    "    positions_all = pd.concat([positions_all, positions], axis=0)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c72c6f00050ddf55",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "positions_all"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "96dd53adf5fbc5ba",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "positions_all.fillna(0)\n",
    "# 找出重复的索引\n",
    "duplicated_index = positions_all.index.duplicated(keep='first')\n",
    "# 删除具有重复索引的列\n",
    "positions_all = positions_all[~duplicated_index]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "556385689580c4a8",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# 将position_all中的int类型转换为float类型\n",
    "for col in positions_all.columns:\n",
    "    positions_all[col] = positions_all[col].astype(float)\n",
    "test_signal_df = positions_all.T\n",
    "test_signal_df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "786cf7d54d990d1b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "backtest(positions_all,0.0001,univ02,'2010-05-07','2024-01-25',3,'strategy 2.3 pairs')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "19522750939975b5",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## strategy 1.10 pairs"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8e1b3f17b57a33b9"
  },
  {
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "prices = load_data(univ02, sec_post_1d_df)\n",
    "# prices = prices.dropna()\n",
    "for drift in tqdm(range(0, 5011)):\n",
    "    # merge positions\n",
    "    if drift == 0:\n",
    "        positions = rolling_generate_signals1(prices, drift, 126, 10, 1)\n",
    "        positions_all = positions\n",
    "        continue\n",
    "    else:\n",
    "        positions = rolling_generate_signals1(prices, drift, 126, 10, 0)\n",
    "    positions_all = pd.concat([positions_all, positions], axis=0)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2cc74578655856cd",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "positions_all"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "469cb1812d82acb5",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "positions_all.fillna(0)\n",
    "# 找出重复的索引\n",
    "duplicated_index = positions_all.index.duplicated(keep='first')\n",
    "# 删除具有重复索引的列\n",
    "positions_all = positions_all[~duplicated_index]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "76605a39e4d5b162",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# 将position_all中的int类型转换为float类型\n",
    "for col in positions_all.columns:\n",
    "    positions_all[col] = positions_all[col].astype(float)\n",
    "test_signal_df = positions_all.T\n",
    "test_signal_df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "52a454d7be4c99ef",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "backtest(positions_all,0.0001,univ02,'2010-05-07','2024-01-25',10,'strategy 1.10 pairs')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bc9e4398bff641dc",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## strategy 2.10 pairs"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e02c017fb935e9c8"
  },
  {
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "prices = load_data(univ02, sec_post_1d_df)\n",
    "# prices = prices.dropna()\n",
    "for drift in tqdm(range(0, 5011)):\n",
    "    # merge positions\n",
    "    if drift == 0:\n",
    "        positions = rolling_generate_signals2(prices, drift, 126, 10, 1)\n",
    "        positions_all = positions\n",
    "        continue\n",
    "    else:\n",
    "        positions = rolling_generate_signals2(prices, drift, 126, 10, 0)\n",
    "    positions_all = pd.concat([positions_all, positions], axis=0)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b7dd06a032815a38",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "positions_all"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8ba293a04bbee1a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "positions_all.fillna(0)\n",
    "# 找出重复的索引\n",
    "duplicated_index = positions_all.index.duplicated(keep='first')\n",
    "# 删除具有重复索引的列\n",
    "positions_all = positions_all[~duplicated_index]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f00cf7b47e89003a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# 将position_all中的int类型转换为float类型\n",
    "for col in positions_all.columns:\n",
    "    positions_all[col] = positions_all[col].astype(float)\n",
    "test_signal_df = positions_all.T\n",
    "test_signal_df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "76eb61ca81eda716",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "backtest(positions_all,0.0001,univ02,'2010-05-07','2024-01-25',10,'strategy 2.10 pairs')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "17dbe173f921fc51",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
